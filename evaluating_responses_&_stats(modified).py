# -*- coding: utf-8 -*-
"""Evaluating Responses & Stats(SE)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a9rtYPn7YpDCvYwzr-PVK8BpI5MOz_SS

# Pre-processing
"""

import pandas as pd
import re

def find_unique_tags(file_path, column_name='Question Tags'):
    # Read the Excel file
    df = pd.read_excel(file_path)

    # Extract the column containing question tags
    question_tags = df[column_name]

    # Initialize an empty list to store all tags
    all_tags = []

    # Iterate through each cell in the column
    for tags in question_tags.dropna():
        # Find all tags enclosed within <>
        tags_list = re.findall(r'<(.*?)>', tags)
        all_tags.extend(tags_list)

    # Create a Series from the list of tags and count unique tags
    unique_tags = pd.Series(all_tags).value_counts()

    return unique_tags

def write_tag_counts_to_file(tag_counts, output_file):
    with open(output_file, 'w') as f:
        for tag, count in tag_counts.items():
            f.write(f"{tag}: {count}\n")

# Example usage
file_path = 'Final Processed Dataset.xlsx'
output_file = 'tag_counts.txt'

unique_tags = find_unique_tags(file_path)
write_tag_counts_to_file(unique_tags, output_file)
print("Tag counts have been written to", output_file)

from lxml import etree
import pandas as pd

def extract_data_from_xml(xml_file):
    parser = etree.XMLParser(recover=True)
    tree = etree.parse(xml_file, parser=parser)
    root = tree.getroot()

    data = []

    for row in root.findall('.//row'):
        accepted_answer_id = row.get('AcceptedAnswerId')

        if accepted_answer_id:
            question_id = row.get('Id')
            question_title = row.get('Title')
            question_body = row.get('Body')
            question_tags = row.get('Tags')

            accepted_answer_row = root.find(f'.//row[@Id="{accepted_answer_id}"]')

            if accepted_answer_row is not None:
                accepted_answer_body = accepted_answer_row.get('Body')
                data.append({
                    'Question Title': question_title,
                    'Question Body': question_body,
                    'Question Tags': question_tags,
                    'Accepted Answer Body': accepted_answer_body
                })

    return data

def save_to_excel(data, output_file):
    df = pd.DataFrame(data)
    df.to_excel(output_file, index=False)

# Replace 'input.xml' and 'output.xlsx' with your actual file paths
input_xml_file = 'Posts.xml'
output_excel_file = 'output.xlsx'

# Extract data from XML and save to Excel
data = extract_data_from_xml(input_xml_file)
save_to_excel(data, output_excel_file)

!pip install openpyxl
!pip install pandas beautifulsoup4
import pandas as pd
from bs4 import BeautifulSoup

def html_to_text(html):
    """Convert HTML to plain text."""
    soup = BeautifulSoup(html, 'html.parser')
    # Find all anchor tags
    for a_tag in soup.find_all('a'):
        # Replace the anchor tag with its href attribute (link)
        a_tag.replace_with(a_tag.get('href'))
    return soup.get_text()

def convert_excel_html_to_text(file_path):
    """Convert HTML content in Excel cells to plain text."""
    # Read the Excel file into a pandas DataFrame
    df = pd.read_excel(file_path)

    # Iterate through each cell in the DataFrame
    for index, row in df.iterrows():
        for column in df.columns:
            cell_value = row[column]
            if isinstance(cell_value, str) and '<' in cell_value:
                # If the cell contains HTML, convert it to plain text
                plain_text = html_to_text(cell_value)
                # Replace the cell value with the plain text
                df.at[index, column] = plain_text

    # Save the modified DataFrame back to an Excel file
    output_file = "output_" + file_path
    df.to_excel(output_file, index=False)
    print(f"Converted HTML content in '{file_path}' to plain text. Output saved to '{output_file}'.")

# Example usage:
if __name__ == "__main__":
    excel_file_path = "xyz.xlsx"  # Provide the path to your Excel file
    convert_excel_html_to_text(excel_file_path)

"""# Selection of Samples Based on Tags"""

import pandas as pd
import random

# Load the Excel file
file_path = 'dataset.xlsx'  # Replace 'your_file.xlsx' with the path to your Excel file
df = pd.read_excel(file_path)

# Define the tags
tags_to_select = ['deep-learning', 'dataset', 'classification', 'statistics', 'clustering']

# Create an empty DataFrame to store the selected samples
selected_df = pd.DataFrame()

# Iterate over each tag
for tag in tags_to_select:
    # Filter the DataFrame to include only rows with the current tag
    tag_df = df[df['Question Tags'].str.contains(tag)]

    # Randomly select 400 samples
    if len(tag_df) > 400:
        tag_df = tag_df.sample(n=400, random_state=42)

    # Append the selected samples to the final DataFrame
    selected_df = pd.concat([selected_df, tag_df], ignore_index=True)

# Output the final dataset
print(selected_df)

selected_df.to_excel('Final_Sample(SE).xlsx', index=False)

import pandas as pd
df = pd.read_excel('Final StackEXchange.xlsx')

df

df['label'] = ''

# Label the first 400 rows as 'deep-learning'
df.loc[:399, 'label'] = 'deep-learning'

# Label the next 400 rows as 'dataset'
df.loc[400:799, 'label'] = 'dataset'

# Label the next 400 rows as 'classification'
df.loc[800:1199, 'label'] = 'classification'

# Label the next 400 rows as 'statistics'
df.loc[1200:1599, 'label'] = 'statistics'

# Label the rest of the rows as 'clustering'
df.loc[1600:, 'label'] = 'clustering'

# Print the DataFrame to verify the labels
df.to_excel('Final StackEXchange.xlsx', index=False)
df

"""# Evaluating the responses of LLMs and Human"""

!pip install pandas
!pip install transformers
!pip install sentence-transformers
!pip install bert-score
!pip install nltk
!pip install bert-score
!pip  install rouge-score

import pandas as pd
from sentence_transformers import SentenceTransformer
from bert_score import BERTScorer
from rouge_score import rouge_scorer
from nltk.translate import meteor_score
from sentence_transformers import SentenceTransformer, util
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

!pip install -U evaluate
!pip install bert_score

import pandas as pd
import evaluate
bertscore = evaluate.load("bertscore")


# df = df[df['label'] == 'classification']

scores_df = pd.DataFrame(columns=['Model Type', 'Cosine Similarity', 'Rouge-L', 'Bleurt', 'Bert'])

"""Comparison among Falco, Mistral and LLama2"""

def cosine_similarity_score(sentences1, sentences2):
    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
    embeddings1 = model.encode(sentences1, convert_to_tensor=True)
    embeddings2 = model.encode(sentences2, convert_to_tensor=True)
    cos_scores = util.pytorch_cos_sim(embeddings1, embeddings2)
    return cos_scores.numpy().diagonal()

def rouge_l_score(sentences1, sentences2):
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    rouge_scores = [scorer.score(s1, s2)['rougeL'].fmeasure for s1, s2 in zip(sentences1, sentences2)]
    return rouge_scores

def bert_score(sentences1, sentences2):
    results = bertscore.compute(predictions=[sentences1], references=[sentences2],model_type='bert-base-uncased', lang="en")

    return results['f1'][0]

def calculate_meteor_score(reference, candidate):
    return meteor_score([reference], candidate)


def bleurt_score(reference_texts, candidate_texts, model_name="Elron/bleurt-large-512", max_length=512):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    # Compute BLEURT score
    with torch.no_grad():
        tokenized_inputs = tokenizer(reference_texts, candidate_texts, truncation=True, max_length=max_length, padding=True, return_tensors='pt')
        scores = model(**tokenized_inputs)[0].squeeze()
    return scores

def compare_responses(df):
    # Initialize DataFrame to store scores
    scores_df = pd.DataFrame(columns=['Model Type', 'Cosine Similarity', 'Rouge-L', 'Bleurt Score', 'Bert Score'])

    # Iterate over pairs of columns to compare
    pairs_to_compare = [(2, 3), (2, 4), (2, 5), (3, 4), (3, 5), (4, 5)]
    for pair in pairs_to_compare:
        bleurt_sc = []
        # Extract LLM responses for the pair of columns
        col1 = df.iloc[:, pair[0]].tolist()
        col2 = df.iloc[:, pair[1]].tolist()
        bleurt_sc = []
        bert_sc = []


        for i in range(0,len(col1)):
          print(i)
          bleurt_sc.append(bleurt_score(col1[i], col2[i]))
          bert_sc.append(bert_score(col1[i], col2[i]))

        # Compute scores
        cosine_scores = cosine_similarity_score(col1, col2)
        rouge_l_scores = rouge_l_score(col1, col2)


        # Calculate average scores
        avg_cosine_similarity = sum(cosine_scores) / len(cosine_scores)
        avg_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)
        avg_bleurt = sum(bleurt_sc) / len(bleurt_sc)
        avg_bert = sum(bert_sc) / len(bert_sc)
        print(pair, avg_cosine_similarity, avg_rouge_l)

        # Append scores to DataFrame
        scores_df = scores_df.append({
            'Model Type': f'Columns {pair[0]} & {pair[1]}',
            'Cosine Similarity': avg_cosine_similarity,
            'Rouge-L': avg_rouge_l,
            'Bleurt Score': avg_bleurt,
            'Bert': avg_bert

        }, ignore_index=True)

    return scores_df

excel_file = 'Final StackEXchange - Processed.xlsx'

df = pd.read_excel(excel_file)

unique_labels = df['label'].unique()

for col in df.columns:
    df[col] = df[col].astype(str)


# Traverse unique labels
for label in unique_labels:
    x_frame = df[df['label'] == label]
    print(label)
    human_responses = x_frame['response'].tolist()
    mistral_responses = x_frame['mistral_response'].tolist()
    falcon_responses = x_frame['falcon_response'].tolist()
    Llama2_responses = x_frame['llama2_response'].tolist()
    sc = compare_responses(x_frame)
    print(sc)

sc = compare_responses(x_frame)
sc



sc



import pandas as pd
excel_file = 'Response_200_Samples(Reddit Data).xlsx'

df = pd.read_excel(excel_file)
df.head()



scores_df = pd.DataFrame(columns=['Model Type', 'Cosine Similarity', 'Rouge-L'])

human_responses = df['Accepted Answer Body'].tolist()
mistral_responses = df['Mistral'].tolist()
falcon_responses = df['Falcon'].tolist()
Llama2_responses = df['Llama-2'].tolist()



# Define a function to compute Cosine Similarity using sentence-transformers
def cosine_similarity_score(sentences1, sentences2):
    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')
    embeddings1 = model.encode(sentences1, convert_to_tensor=True)
    embeddings2 = model.encode(sentences2, convert_to_tensor=True)
    cos_scores = util.pytorch_cos_sim(embeddings1, embeddings2)
    return cos_scores.numpy().diagonal()

# Define a function to compute BLEURT score using BERTScore
def bleurt_score(sentences1, sentences2):
    scorer = BERTScorer(lang="en")
    _, _, bleurt_scores = scorer.score(sentences1, sentences2)
    return bleurt_scores.numpy()

# Define a function to compute Rouge-L score
def rouge_l_score(sentences1, sentences2):
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    rouge_scores = [scorer.score(s1, s2)['rougeL'].fmeasure for s1, s2 in zip(sentences1, sentences2)]
    return rouge_scores

# Define a function to compute BERTScore
def bert_score(sentences1, sentences2):
    scorer = BERTScorer(lang="en")
    _, _, bert_scores = scorer.score(sentences1, sentences2)
    return bert_scores.numpy()


mistral_cosine_scores = cosine_similarity_score(human_responses, mistral_responses)
# mistral_bleurt_scores = bleurt_score(human_responses, mistral_responses)
mistral_rouge_l_scores = rouge_l_score(human_responses, mistral_responses)
# mistral_bert_scores = bert_score(human_responses, mistral_responses)

# Compute the average score for each metric
avg_mistral_cosine_similarity = sum(mistral_cosine_scores) / len(mistral_cosine_scores)
# avg_mistral_bleurt = sum(mistral_bleurt_scores) / len(mistral_bleurt_scores)
avg_mistral_rouge_l = sum(mistral_rouge_l_scores) / len(mistral_rouge_l_scores)
# avg_mistral_bert_score = sum(mistral_bert_scores) / len(mistral_bert_scores)



# Append a row to the DataFrame
scores_df = scores_df.append({
    'Model Type': "Mixtral",
    'Cosine Similarity': avg_mistral_cosine_similarity,
    # 'BLEURT': avg_mistral_bleurt,
    'Rouge-L': avg_mistral_rouge_l
}, ignore_index=True)



falcon_responses = [element for element in falcon_responses if not isinstance(element, float)]

falcon_cosine_scores = cosine_similarity_score(human_responses, falcon_responses)
# falcon_bleurt_scores = bleurt_score(human_responses, falcon_responses)
falcon_rouge_l_scores = rouge_l_score(human_responses, falcon_responses)
# falcon_bert_scores = bert_score(human_responses, falcon_responses)


# Compute the average score for each metric
avg_falcon_cosine_similarity = sum(falcon_cosine_scores) / len(falcon_cosine_scores)
# avg_falcon_bleurt = sum(falcon_bleurt_scores) / len(falcon_bleurt_scores)
avg_falcon_rouge_l = sum(falcon_rouge_l_scores) / len(falcon_rouge_l_scores)
# avg_falcon_bert_score = sum(falcon_bert_scores) / len(falcon_bert_scores)


# Append a row to the DataFrame
scores_df = scores_df.append({
   'Model Type': "Falcon",
   'Cosine Similarity': avg_falcon_cosine_similarity,
  #  'BLEURT': avg_falcon_bleurt,
   'Rouge-L': avg_falcon_rouge_l,
  #  'BERTScore': avg_falcon_bert_score
}, ignore_index=True)



Llama2_responses = [element for element in Llama2_responses if not isinstance(element, float)]

Llama2_cosine_scores = cosine_similarity_score(human_responses, Llama2_responses)
# Llama2_bleurt_scores = bleurt_score(human_responses, Llama2_responses)
Llama2_rouge_l_scores = rouge_l_score(human_responses, Llama2_responses)
# Llama2_bert_scores = bert_score(human_responses, Llama2_responses)


# Compute the average score for each metric
avg_Llama2_cosine_similarity = sum(Llama2_cosine_scores) / len(Llama2_cosine_scores)
# avg_Llama2_bleurt = sum(Llama2_bleurt_scores) / len(Llama2_bleurt_scores)
avg_Llama2_rouge_l = sum(Llama2_rouge_l_scores) / len(Llama2_rouge_l_scores)
# avg_Llama2_bert_score = sum(Llama2_bert_scores) / len(Llama2_bert_scores)


# Append a row to the DataFrame
scores_df = scores_df.append({
   'Model Type': "llama2",
   'Cosine Similarity': avg_Llama2_cosine_similarity,
   'Rouge-L': avg_Llama2_rouge_l,
}, ignore_index=True)


print(scores_df)

import matplotlib.pyplot as plt
import numpy as np

# Data
model_types = ['Mixtral', 'Falcon', 'llama2']
stackexchange_cosine_similarity = [0.558765, 0.449029, 0.303240]
stackexchange_rouge_l = [0.131091, 0.124078, 0.112549]
reddit_cosine_similarity = [0.325585, 0.216017, 0.313956]
reddit_rouge_l = [0.067888, 0.067345, 0.072582]

x = np.arange(len(model_types))  # the label locations
width = 0.35  # the width of the bars

# Plotting StackExchange data
fig, ax = plt.subplots(figsize=(8, 6))
rects1 = ax.bar(x - width/2, stackexchange_cosine_similarity, width, label='Cosine Similarity')
rects2 = ax.bar(x + width/2, stackexchange_rouge_l, width, label='Rouge-L')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Score')
ax.set_xticks(x)
ax.set_xticklabels(model_types)
ax.legend()
ax.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()

def autolabel(rects):
    """Attach a text label above each bar in *rects*, displaying its height."""
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(height),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

autolabel(rects1)
autolabel(rects2)

plt.show()

# Plotting Reddit data
fig, ax = plt.subplots(figsize=(8, 6))
rects1 = ax.bar(x - width/2, reddit_cosine_similarity, width, label='Cosine Similarity')
rects2 = ax.bar(x + width/2, reddit_rouge_l, width, label='Rouge-L')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Score')
ax.set_xticks(x)
ax.set_xticklabels(model_types)
ax.legend()
ax.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()

autolabel(rects1)
autolabel(rects2)

plt.show()

"""# Desciptive Stats"""

import xml.etree.ElementTree as ET
from datetime import datetime

# Function to parse datetime strings
def parse_datetime(datetime_str):
    return datetime.fromisoformat(datetime_str)

# Function to find date range
def find_date_range(dates):
    if not dates:
        return None, None
    return min(dates), max(dates)

# Load XML file
tree = ET.parse('Posts.xml')
root = tree.getroot()

# Initialize variables
post_count = 0
comment_count = 0
post_dates = []
comment_dates = []

# Iterate through each row element
for row in root.findall('row'):
    post_type = row.get('PostTypeId')
    creation_date = row.get('CreationDate')
    comment_count += int(row.get('CommentCount', 0))

    if post_type == '1' and row.get('ParentId') is None:
        post_count += 1
        post_dates.append(parse_datetime(creation_date))

    if row.get('CommentCount'):
        comment_dates.extend([parse_datetime(creation_date)] * int(row.get('CommentCount')))

# Find date ranges
posts_start_date, posts_end_date = find_date_range(post_dates)
comments_start_date, comments_end_date = find_date_range(comment_dates)

# Print results
print("Number of posts:", post_count)
print("Number of comments:", comment_count)
print("Posts date range:", posts_start_date, "-", posts_end_date)
print("Comments date range:", comments_start_date, "-", comments_end_date)

# Initialize variables
unique_owner_user_ids = set()

# Iterate through each row element
for row in root.findall('row'):
    owner_user_id = row.get('OwnerUserId')
    if owner_user_id:
        unique_owner_user_ids.add(owner_user_id)

# Print unique OwnerUserIds
print("Unique OwnerUserIds:", len(unique_owner_user_ids))

"""# post processing

Deleting Unncessary Lines
"""

import re

# Define the regular expression pattern to match lines starting with a number
pattern = re.compile(r'^\d')

# Function to remove lines starting with a number
def remove_lines(file_path):
    with open(file_path, 'r') as file:
        lines = file.readlines()

    # Filter out lines starting with a number
    modified_lines = [line for line in lines if not pattern.match(line)]

    # Write the modified content back to the file
    with open(file_path, 'w') as file:
        file.writelines(modified_lines)

# Example usage:
file_path = 'abc.txt'  # Replace 'example.txt' with the path to your text file
remove_lines(file_path)
print("Lines starting with a number have been removed from the file.")

"""## Reddit Data"""

import pandas as pd

# Define data for each dataframe
data1 = {
    'Model Type': ['Columns 2 & 3', 'Columns 2 & 4', 'Columns 2 & 5', 'Columns 3 & 4', 'Columns 3 & 5', 'Columns 4 & 5'],
    'Cosine Similarity': [0.338811, 0.313406, 0.330817, 0.739113, 0.791091, 0.727836],
    'Rouge-L': [0.075887, 0.073646, 0.077736, 0.197274, 0.234149, 0.208611],
}

data2 = {
    'Model Type': ['Columns 2 & 3', 'Columns 2 & 4', 'Columns 2 & 5', 'Columns 3 & 4', 'Columns 3 & 5', 'Columns 4 & 5'],
    'Cosine Similarity': [0.314391, 0.291137, 0.303545, 0.762987, 0.809600, 0.764462],
    'Rouge-L': [0.077874, 0.076590, 0.078576, 0.204828, 0.240694, 0.218840],
}

data3 = {
    'Model Type': ['Columns 2 & 3', 'Columns 2 & 4', 'Columns 2 & 5', 'Columns 3 & 4', 'Columns 3 & 5', 'Columns 4 & 5'],
    'Cosine Similarity': [0.324820, 0.297393, 0.317902, 0.726753, 0.778734, 0.719945],
    'Rouge-L': [0.069727, 0.072251, 0.072829, 0.195578, 0.238551, 0.207968],
}

data4 = {
    'Model Type': ['Columns 2 & 3', 'Columns 2 & 4', 'Columns 2 & 5', 'Columns 3 & 4', 'Columns 3 & 5', 'Columns 4 & 5'],
    'Cosine Similarity': [0.310762, 0.281667, 0.307206, 0.712640, 0.772468, 0.721455],
    'Rouge-L': [0.065035, 0.063856, 0.067211, 0.200643, 0.239490, 0.217274],
}

data5 = {
    'Model Type': ['Columns 2 & 3', 'Columns 2 & 4', 'Columns 2 & 5', 'Columns 3 & 4', 'Columns 3 & 5', 'Columns 4 & 5'],
    'Cosine Similarity': [0.345462, 0.314587, 0.335518, 0.682249, 0.752948, 0.685638],
    'Rouge-L': [0.078876, 0.077374, 0.079224, 0.193518, 0.231758, 0.204162],
}

# Convert each dictionary to a DataFrame and add 'label' column
df1 = pd.DataFrame(data1)
df1['label'] = 'deep learning'

df2 = pd.DataFrame(data2)
df2['label'] = 'clustering'

df3 = pd.DataFrame(data3)
df3['label'] = 'data science'

df4 = pd.DataFrame(data4)
df4['label'] = 'classification'

df5 = pd.DataFrame(data5)
df5['label'] = 'statistics'

# Concatenate the DataFrames
frames = [df1, df2, df3, df4, df5]
result_df = pd.concat(frames, ignore_index=True)

print(result_df)

result_df['Model Type'].replace({
    'Columns 2 & 3': 'Human vs Mistral',
    'Columns 2 & 4': 'Human vs Falcon',
    'Columns 2 & 5': 'Human vs Llama2',
    'Columns 3 & 4': 'Mistral vs Falcon',
    'Columns 3 & 5': 'Mistral vs Llama2',
    'Columns 4 & 5': 'Falcon vs Llama2'
}, inplace=True)

result_df